{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMutQMkcRVDs9vqRMytNYFm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bcf59fffb11b4b069d2a9cfffab6b817":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b8f071cb25134372bf46178d2f48c03f","IPY_MODEL_2cbf573f74374464b43649b1810e391d","IPY_MODEL_70695a47ac50442fb77d63f068489891"],"layout":"IPY_MODEL_899f7cf6a71248dc9a3c2f4fb64bfcc8"}},"b8f071cb25134372bf46178d2f48c03f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e70f99e81c6f45ee871da5114148edd7","placeholder":"​","style":"IPY_MODEL_45cfd27e935940cda8445f961be85c58","value":"Map: 100%"}},"2cbf573f74374464b43649b1810e391d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_961c9153ff804d20910f6b0e5bdb12fb","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_489685969e87415183bbc8cea477338a","value":5000}},"70695a47ac50442fb77d63f068489891":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa9578397c7e413b9152fdf3941bb2bb","placeholder":"​","style":"IPY_MODEL_7324e765a9024280944ef2717dbb2e93","value":" 5000/5000 [00:23&lt;00:00, 219.39 examples/s]"}},"899f7cf6a71248dc9a3c2f4fb64bfcc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e70f99e81c6f45ee871da5114148edd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45cfd27e935940cda8445f961be85c58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"961c9153ff804d20910f6b0e5bdb12fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"489685969e87415183bbc8cea477338a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa9578397c7e413b9152fdf3941bb2bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7324e765a9024280944ef2717dbb2e93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uu8iY4D25cRv","executionInfo":{"status":"ok","timestamp":1754497840926,"user_tz":-330,"elapsed":16864,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}},"outputId":"83cd2138-5181-4489-a83a-41a656007648"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Cell 1: Install dependencies\n","!pip install -q transformers datasets rouge_score evaluate"]},{"cell_type":"code","source":["# Cell 2: Import libraries\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Trainer\n","import evaluate\n","import numpy as np"],"metadata":{"id":"RGN4b0Me6DjK","executionInfo":{"status":"ok","timestamp":1754497908260,"user_tz":-330,"elapsed":19379,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Cell 3: Load dataset and manually select small subset\n","full_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n","dataset = {\n","    \"train\": full_dataset[\"train\"].select(range(5000)),         # first 1000 samples\n","    \"validation\": full_dataset[\"validation\"].select(range(200)),\n","    \"test\": full_dataset[\"test\"].select(range(200)),\n","}"],"metadata":{"id":"-pc71UW46HfX","executionInfo":{"status":"ok","timestamp":1754499696769,"user_tz":-330,"elapsed":5435,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Cell 4: Load tokenizer and model (T5-small for speed)\n","model_name = \"t5-small\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"],"metadata":{"id":"p1rIZ5wb6IpG","executionInfo":{"status":"ok","timestamp":1754499701090,"user_tz":-330,"elapsed":1996,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Cell 5: Preprocessing parameters\n","max_input_length = 512\n","max_target_length = 128\n","\n","def preprocess(ex):\n","    # Join list of articles into one string if needed\n","    article = \" \".join(ex[\"article\"]) if isinstance(ex[\"article\"], list) else ex[\"article\"]\n","\n","    # Prefix for T5 summarization\n","    src = \"summarize: \" + article\n","    tgt = ex[\"highlights\"]\n","\n","    # Tokenize source and target\n","    inp = tokenizer(src, max_length=max_input_length, truncation=True, padding=\"max_length\")\n","    tgt_tok = tokenizer(\n","        tgt,\n","        max_length=max_target_length,\n","        truncation=True,\n","        padding=\"max_length\"\n","    )\n","\n","    # Replace pad tokens in labels with -100\n","    labels = tgt_tok[\"input_ids\"]\n","    labels = [label if label != tokenizer.pad_token_id else -100 for label in labels]\n","\n","    inp[\"labels\"] = labels\n","\n","    return inp\n","\n","# Apply preprocessing to each split in the dataset dict\n","tokenized = {\n","    split: ds.map(preprocess, batched=False, remove_columns=ds.column_names)\n","    for split, ds in dataset.items()\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["bcf59fffb11b4b069d2a9cfffab6b817","b8f071cb25134372bf46178d2f48c03f","2cbf573f74374464b43649b1810e391d","70695a47ac50442fb77d63f068489891","899f7cf6a71248dc9a3c2f4fb64bfcc8","e70f99e81c6f45ee871da5114148edd7","45cfd27e935940cda8445f961be85c58","961c9153ff804d20910f6b0e5bdb12fb","489685969e87415183bbc8cea477338a","aa9578397c7e413b9152fdf3941bb2bb","7324e765a9024280944ef2717dbb2e93"]},"id":"3DxFlwmN-TE1","executionInfo":{"status":"ok","timestamp":1754499728003,"user_tz":-330,"elapsed":23628,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}},"outputId":"08dcdf6e-890d-4a28-eec1-9ed0f53cf3cf"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcf59fffb11b4b069d2a9cfffab6b817"}},"metadata":{}}]},{"cell_type":"code","source":["# Cell 6: Training setup\n","batch_size = 4\n","args = Seq2SeqTrainingArguments(\n","    output_dir=\"summarizer\",\n","    eval_strategy=\"no\",\n","    learning_rate=3e-5,\n","    gradient_accumulation_steps=batch_size,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=6,\n","    weight_decay=0.01,\n","    save_total_limit=1,\n","    logging_steps=10,\n","    predict_with_generate=True,\n","    report_to=\"none\",\n","    eval_accumulation_steps=10,\n","    generation_max_length=64,\n","    generation_num_beams=1,\n","    push_to_hub=False,\n","    load_best_model_at_end=False,\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"],"metadata":{"id":"75bvpbin8b6K","executionInfo":{"status":"ok","timestamp":1754499731649,"user_tz":-330,"elapsed":20,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Cell 7: Clear memory\n","from transformers import TrainerCallback\n","\n","class ClearMemoryCallback(TrainerCallback):\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        import gc, torch\n","        gc.collect()\n","        torch.cuda.empty_cache()"],"metadata":{"id":"_rfp9ETNcejX","executionInfo":{"status":"ok","timestamp":1754499733894,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Cell 8: Train\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized[\"train\"],\n","    eval_dataset=tokenized[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=None,\n","    callbacks=[ClearMemoryCallback()],\n",")\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"eBHuoGve8htj","outputId":"da3af749-bd9e-4c7b-8850-988b3b8757a6","collapsed":true,"executionInfo":{"status":"ok","timestamp":1754501108574,"user_tz":-330,"elapsed":1369055,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-189983027.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1878' max='1878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1878/1878 22:47, Epoch 6/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>2.607700</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>2.515500</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>2.284800</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>2.408600</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.205400</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>2.217600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>2.274900</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>2.309700</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>2.223500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>2.213300</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>2.214500</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>2.183200</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>2.132500</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>2.221300</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>2.151600</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>2.164200</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>2.246800</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>2.145300</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>2.191300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>2.127300</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>2.099900</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>2.208100</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>2.189300</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>2.230900</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>2.201100</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>2.196100</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>2.088900</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>2.165900</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>2.243200</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>2.128600</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>2.243200</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>2.091900</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>2.088400</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>2.136100</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>2.139700</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>2.128300</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>2.141200</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>2.117600</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>2.180900</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>2.176500</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>2.199100</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>2.192400</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>2.211600</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>2.158000</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>2.115200</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>2.134300</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>2.073600</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>2.056200</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>2.177300</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>2.240100</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>2.274400</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>2.084400</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>2.159400</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>2.123500</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>2.147500</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>2.147700</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>2.129800</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>2.025500</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>2.213500</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>2.173500</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>2.125100</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>2.084300</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>2.017900</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>2.155400</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>2.130100</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>2.108800</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>2.077000</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>2.132000</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>2.078100</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>2.096200</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>2.032100</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>2.158600</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>2.055400</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>2.133100</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>2.182200</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>2.027000</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>2.086300</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>2.007300</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>2.104000</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>2.088400</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>2.204300</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>2.098700</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>2.154700</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>2.195500</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>2.197000</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>2.149600</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>2.082600</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>2.120600</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>2.162200</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>2.133700</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>2.079000</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>2.224000</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>2.065900</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>2.062900</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>2.147700</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>2.083100</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>2.061600</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>2.106200</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>2.118500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.176900</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>2.093600</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>1.952800</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>2.105200</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>2.006200</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>2.018500</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>2.092700</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>2.144400</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>2.097200</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>2.123800</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>2.129200</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>2.167000</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>2.086100</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>2.093800</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>2.108000</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>2.051100</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>2.175800</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>2.159500</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>2.172100</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>2.161200</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>2.072600</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>2.069500</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>2.106100</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>2.124400</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>2.018500</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>2.161700</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>2.075400</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>2.062600</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>2.088300</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>2.116900</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>2.099100</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>2.013000</td>\n","    </tr>\n","    <tr>\n","      <td>1320</td>\n","      <td>2.139200</td>\n","    </tr>\n","    <tr>\n","      <td>1330</td>\n","      <td>2.103200</td>\n","    </tr>\n","    <tr>\n","      <td>1340</td>\n","      <td>2.024300</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>2.095100</td>\n","    </tr>\n","    <tr>\n","      <td>1360</td>\n","      <td>2.046000</td>\n","    </tr>\n","    <tr>\n","      <td>1370</td>\n","      <td>2.137200</td>\n","    </tr>\n","    <tr>\n","      <td>1380</td>\n","      <td>2.167600</td>\n","    </tr>\n","    <tr>\n","      <td>1390</td>\n","      <td>2.005700</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>2.229900</td>\n","    </tr>\n","    <tr>\n","      <td>1410</td>\n","      <td>2.137100</td>\n","    </tr>\n","    <tr>\n","      <td>1420</td>\n","      <td>1.978100</td>\n","    </tr>\n","    <tr>\n","      <td>1430</td>\n","      <td>2.167700</td>\n","    </tr>\n","    <tr>\n","      <td>1440</td>\n","      <td>2.141000</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>2.117900</td>\n","    </tr>\n","    <tr>\n","      <td>1460</td>\n","      <td>2.069000</td>\n","    </tr>\n","    <tr>\n","      <td>1470</td>\n","      <td>2.125500</td>\n","    </tr>\n","    <tr>\n","      <td>1480</td>\n","      <td>2.020900</td>\n","    </tr>\n","    <tr>\n","      <td>1490</td>\n","      <td>2.106100</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>2.086900</td>\n","    </tr>\n","    <tr>\n","      <td>1510</td>\n","      <td>2.023400</td>\n","    </tr>\n","    <tr>\n","      <td>1520</td>\n","      <td>2.035900</td>\n","    </tr>\n","    <tr>\n","      <td>1530</td>\n","      <td>2.094800</td>\n","    </tr>\n","    <tr>\n","      <td>1540</td>\n","      <td>2.146700</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>2.055100</td>\n","    </tr>\n","    <tr>\n","      <td>1560</td>\n","      <td>2.159600</td>\n","    </tr>\n","    <tr>\n","      <td>1570</td>\n","      <td>2.013700</td>\n","    </tr>\n","    <tr>\n","      <td>1580</td>\n","      <td>2.151900</td>\n","    </tr>\n","    <tr>\n","      <td>1590</td>\n","      <td>2.125900</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>2.135400</td>\n","    </tr>\n","    <tr>\n","      <td>1610</td>\n","      <td>1.945800</td>\n","    </tr>\n","    <tr>\n","      <td>1620</td>\n","      <td>2.162700</td>\n","    </tr>\n","    <tr>\n","      <td>1630</td>\n","      <td>2.079300</td>\n","    </tr>\n","    <tr>\n","      <td>1640</td>\n","      <td>2.136400</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>2.053900</td>\n","    </tr>\n","    <tr>\n","      <td>1660</td>\n","      <td>2.119800</td>\n","    </tr>\n","    <tr>\n","      <td>1670</td>\n","      <td>2.059100</td>\n","    </tr>\n","    <tr>\n","      <td>1680</td>\n","      <td>1.975000</td>\n","    </tr>\n","    <tr>\n","      <td>1690</td>\n","      <td>2.157100</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>2.115100</td>\n","    </tr>\n","    <tr>\n","      <td>1710</td>\n","      <td>1.975600</td>\n","    </tr>\n","    <tr>\n","      <td>1720</td>\n","      <td>2.187900</td>\n","    </tr>\n","    <tr>\n","      <td>1730</td>\n","      <td>2.061000</td>\n","    </tr>\n","    <tr>\n","      <td>1740</td>\n","      <td>2.133400</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>2.127100</td>\n","    </tr>\n","    <tr>\n","      <td>1760</td>\n","      <td>2.113200</td>\n","    </tr>\n","    <tr>\n","      <td>1770</td>\n","      <td>2.095600</td>\n","    </tr>\n","    <tr>\n","      <td>1780</td>\n","      <td>2.085600</td>\n","    </tr>\n","    <tr>\n","      <td>1790</td>\n","      <td>1.947300</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>2.111600</td>\n","    </tr>\n","    <tr>\n","      <td>1810</td>\n","      <td>2.015900</td>\n","    </tr>\n","    <tr>\n","      <td>1820</td>\n","      <td>2.060500</td>\n","    </tr>\n","    <tr>\n","      <td>1830</td>\n","      <td>2.007100</td>\n","    </tr>\n","    <tr>\n","      <td>1840</td>\n","      <td>2.098200</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>2.179000</td>\n","    </tr>\n","    <tr>\n","      <td>1860</td>\n","      <td>2.025500</td>\n","    </tr>\n","    <tr>\n","      <td>1870</td>\n","      <td>2.117100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1878, training_loss=2.1260751520102015, metrics={'train_runtime': 1367.8907, 'train_samples_per_second': 21.932, 'train_steps_per_second': 1.373, 'total_flos': 4060254044160000.0, 'train_loss': 2.1260751520102015, 'epoch': 6.0})"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Cell 9: Validation\n","from torch.utils.data import DataLoader\n","import torch\n","\n","def evaluate_model(model, tokenizer, dataset, data_collator, batch_size=4):\n","    model.eval()\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        collate_fn=data_collator,\n","    )\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    for batch in dataloader:\n","        # Move to device\n","        batch = {k: v.to(model.device) for k, v in batch.items()}\n","\n","        # Generate summaries\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                input_ids=batch[\"input_ids\"],\n","                attention_mask=batch[\"attention_mask\"],\n","                max_length=64,\n","                num_beams=1\n","            )\n","\n","        # Decode predictions and references\n","        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        labels = batch[\"labels\"].cpu().numpy()\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","        all_preds.extend([p.strip() for p in decoded_preds])\n","        all_labels.extend([l.strip() for l in decoded_labels])\n","\n","        # Optional: clear memory manually\n","        del batch, outputs\n","        torch.cuda.empty_cache()\n","\n","    # Compute ROUGE\n","    rouge_metric = evaluate.load(\"rouge\")\n","    rouge_metric.add_batch(predictions=all_preds, references=all_labels)\n","    results = rouge_metric.compute(use_stemmer=True)\n","    return {k: round(v * 100, 2) for k, v in results.items()}\n","\n","results = evaluate_model(model, tokenizer, tokenized[\"validation\"], data_collator)\n","print(\"Validation ROUGE:\", results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9w0_zGFp2vq","executionInfo":{"status":"ok","timestamp":1754501193898,"user_tz":-330,"elapsed":35423,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}},"outputId":"f02f6881-12d2-4408-ef20-225fbebb7876"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation ROUGE: {'rouge1': np.float64(31.1), 'rouge2': np.float64(12.01), 'rougeL': np.float64(22.15), 'rougeLsum': np.float64(22.2)}\n"]}]},{"cell_type":"code","source":["# Cell 10: Generate a sample\n","sample = dataset[\"test\"][74]\n","src = \"summarize: \" + sample[\"article\"]\n","\n","# Tokenize input\n","inputs = tokenizer(src, return_tensors=\"pt\", truncation=True, max_length=512)\n","\n","# Move model and inputs to same device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","# Generate summary\n","outputs = model.generate(**inputs, max_new_tokens=60)\n","\n","# Decode and print\n","print(\"Generated:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n","print(\"Reference:\", sample[\"highlights\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IB_aKqs78j2z","executionInfo":{"status":"ok","timestamp":1754501873091,"user_tz":-330,"elapsed":778,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}},"outputId":"057c0c33-1a1c-4a35-9ba9-7aca4a2bcc78"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated: Three people killed and five wounded in clashes with armed assailants. A security guard of the provincial attorney general's office among the dead.\n","Reference: Three people killed; five wounded in attack on attorney general's office in Balkh province .\n","Staff and civilians have been rescued as gunmen engaged Afghan security forces .\n"]}]},{"cell_type":"code","source":["# Cell 11: Test\n","results = evaluate_model(model, tokenizer, tokenized[\"test\"], data_collator)\n","print(\"Test ROUGE:\", results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gnHhmdKqSmnP","executionInfo":{"status":"ok","timestamp":1754501331063,"user_tz":-330,"elapsed":35864,"user":{"displayName":"Vijay Subramanya","userId":"13762858695920108145"}},"outputId":"a3cb20ba-d8d5-4d33-8d50-966ba537279d"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Test ROUGE: {'rouge1': np.float64(30.56), 'rouge2': np.float64(10.74), 'rougeL': np.float64(22.04), 'rougeLsum': np.float64(22.1)}\n"]}]}]}